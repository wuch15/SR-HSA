{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "files=['task3_trainingset1_download_form.txt','task3_trainingset2_download_form.txt','task3_trainingset3_download_form.txt']\n",
    "data={}\n",
    "for filename in files:\n",
    "    with open(filename,'r') as f:\n",
    "        for x in f.readlines():\n",
    "            if len(x)>1:\n",
    "                #print(x)\n",
    "                data[int(x.split('\\t')[0])] =  int(x.replace('\\n','').split('\\t')[-1])\n",
    "#print(data)\n",
    "crawl={}\n",
    "\n",
    "with open('tweet_by_ID_23_7_2018__03_55_00.txt','r',encoding='utf-8',errors='ignore') as f:\n",
    "    for x in f.readlines():\n",
    "        tp=JSONDecoder().decode(x)\n",
    "        crawl[tp['id']]=tp['text'].lower()\n",
    "#print(crawl)\n",
    "alldata=[]\n",
    "alllabel=[]\n",
    "for i in crawl:\n",
    "    if i in  data :\n",
    "        alldata.append([wnl.lemmatize(x) for x in tknzr.tokenize(crawl[i])])\n",
    "        alllabel.append(data[i])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict0={'PADDING':[0,99999]}\n",
    "char_dict={'PADDING':0}\n",
    "for i in alldata:\n",
    "    \n",
    "    for word in i:\n",
    "        if word not in word_dict0:\n",
    "            word_dict0[word]=[len(word_dict0),1]\n",
    "        else:\n",
    "            word_dict0[word][1]+=1\n",
    "        for char in word:\n",
    "            if char not in char_dict:\n",
    "                char_dict[char]=len(char_dict)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':[0,99999]}\n",
    "for i in word_dict0:\n",
    "    if word_dict0[i][1]>=2 and i not in word_dict:\n",
    "        word_dict[i]=[len(word_dict),word_dict0[i][1]]\n",
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_idword=[]\n",
    "\n",
    "all_idchar=[]\n",
    "for i in alldata:\n",
    "    words=[x for x in i if x in word_dict]\n",
    "\n",
    "    idword=[]\n",
    "    idchar=[]\n",
    "    \n",
    "    \n",
    "    for word in words:\n",
    "\n",
    "        idword.append(word_dict[word][0])\n",
    "        tp=[char_dict[m] for m in word][:15]\n",
    "        idchar.append(tp+[0]*(15-len(tp)))\n",
    " \n",
    "        \n",
    "    idword=idword[:70]\n",
    "    idchar=idchar[:70]\n",
    "\n",
    "    all_idword.append(idword+[0]*(70-len(idword)))\n",
    "    all_idchar.append(idchar+[[0]*15]*(70-len(idchar)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idword=np.array(all_idword,dtype='float32')\n",
    "all_idchar=np.array(all_idchar,dtype='float32')\n",
    "alllabel=np.array(alllabel,dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index=list(range(len(alllabel)))\n",
    "random.shuffle(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idword.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=int(len(all_idword)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idword=np.array(all_idword,dtype='float32')[index[:nb]]\n",
    "train_idchar=np.array(all_idchar,dtype='float32')[index[:nb]]\n",
    "trainlabel=np.array(alllabel,dtype='float32')[index[:nb]]\n",
    "\n",
    "\n",
    "dev_idword=np.array(all_idword,dtype='float32')[index[nb:]]\n",
    "dev_idchar=np.array(all_idchar,dtype='float32')[index[nb:]]\n",
    "devlabel=np.array(alllabel,dtype='float32')[index[nb:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "embdict=dict()\n",
    "plo=0\n",
    "with open('/data1/wuch/word2vec_twitter_model.bin','rb')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1).decode('utf-8','ignore')\n",
    "            if ch ==' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if len(word) != 0:\n",
    "            \n",
    "            tp= np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            if word in word_dict:\n",
    "                embdict[word]=tp.tolist()\n",
    "                if plo %1000==0:\n",
    "                    print(plo,line,word)\n",
    "                plo+=1\n",
    "                #print(word,tp)\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "from numpy.linalg import cholesky\n",
    "print(len(embdict),len(word_dict))\n",
    "print(len(word_dict))\n",
    "lister=[0]*len(word_dict)\n",
    "xp=np.zeros(400,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict.keys():\n",
    "    lister[word_dict[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(lister[word_dict[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(lister)):\n",
    "    if type(lister[i])==int:\n",
    "        lister[i]=np.reshape(norm, 400)\n",
    "lister[0]=np.zeros(400,dtype='float32')\n",
    "lister=np.array(lister,dtype='float32')\n",
    "print(lister.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, concatenate\n",
    "from keras.layers import Dense, Input, Flatten, average,Lambda\n",
    "from keras.layers import Convolution1D, MaxPooling1D,Conv1D,GlobalMaxPooling1D, AveragePooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers #keras2\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=70\n",
    "MAX_CHAR_LENGTH=15\n",
    "all_results2=[]\n",
    "all_pred=[]\n",
    "for k in range(5):\n",
    "    pos_index=[]\n",
    "    neg_index=[]\n",
    "    for i in range(len(trainlabel)):\n",
    "        if trainlabel[i]==1:\n",
    "            pos_index.append(i)\n",
    "        if trainlabel[i]==0:\n",
    "            neg_index.append(i)\n",
    "    new_pos_index=list(pos_index)*9+neg_index\n",
    "    random.shuffle(new_pos_index)\n",
    "    new_train_idword=np.array(train_idword,dtype='float32')[new_pos_index]\n",
    "    new_train_idchar=np.array(train_idchar,dtype='float32')[new_pos_index]\n",
    "    new_trainlabel=np.array(trainlabel,dtype='float32')[new_pos_index]\n",
    "\n",
    "    embedding_layer = Embedding(len(word_dict),400,input_length=MAX_SENT_LENGTH,weights=[lister], trainable=True)\n",
    "    \n",
    "    \n",
    "    review_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    review_embedded_sequences = embedding_layer(review_input)\n",
    "    d_review_emb=Dropout(0.2)(review_embedded_sequences)\n",
    "\n",
    "    \n",
    "    charembedding_layer = Embedding(len(char_dict),100,input_length=MAX_CHAR_LENGTH,trainable=True)\n",
    "    \n",
    "    char_input = Input(shape=(MAX_CHAR_LENGTH,), dtype='int32')\n",
    "    char_embedded_sequences = charembedding_layer(char_input)\n",
    "    d_char_emb=Dropout(0.2)(char_embedded_sequences)\n",
    "\n",
    "    review_c_cnn_char = Convolution1D(nb_filter=200, filter_length=3, padding='same', activation='relu', strides=1)(d_char_emb)\n",
    "    review_d_c_cnn_char = Dropout(0.2)(review_c_cnn_char)\n",
    "    \n",
    "    review_c_att0=Attention(16,16)([review_d_c_cnn_char,review_d_c_cnn_char,review_d_c_cnn_char])\n",
    "\n",
    "    \n",
    "    \n",
    "    review_c_att = GlobalMaxPooling1D()(review_c_att0)\n",
    "    charEncoder = Model(char_input, review_c_att)\n",
    "    \n",
    "    all_char_input = Input(shape=(MAX_SENT_LENGTH, MAX_CHAR_LENGTH), dtype='int32')\n",
    "    char_encoder = TimeDistributed(charEncoder)(all_char_input)\n",
    "\n",
    "    \n",
    "    all_concat=concatenate([d_review_emb,char_encoder])\n",
    "        \n",
    "\n",
    "    \n",
    "    review_c_cnn_sent = Convolution1D(nb_filter=200, filter_length=3, padding='same', activation='relu', strides=1)(all_concat)\n",
    "    review_d_c_cnn_sent = Dropout(0.2)(review_c_cnn_sent)\n",
    "\n",
    "    review_w_att0=Attention(16,16)([review_d_c_cnn_sent,review_d_c_cnn_sent,review_d_c_cnn_sent])\n",
    "    \n",
    "    review_w_att = GlobalMaxPooling1D()(review_w_att0)\n",
    "    \n",
    "    \n",
    "    all_dense = Dense(200, activation='relu')(review_w_att)   \n",
    "    classifier = Dense(2, activation='softmax')(all_dense)\n",
    "    \n",
    "    \n",
    "    model = Model([review_input,all_char_input], classifier)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    for i in range(1):\n",
    "        model.fit([new_train_idword,new_train_idchar],to_categorical(new_trainlabel),shuffle=True,\n",
    "                  class_weight='auto', batch_size=64, epochs=1,verbose=1)\n",
    "        \n",
    "        y_pred = model.predict([dev_idword,dev_idchar], batch_size=64, verbose=1)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = devlabel\n",
    "        cr = classification_report(y_true, y_pred,digits=4)\n",
    "        print(accuracy_score(y_true, y_pred))\n",
    "        print(cr)\n",
    "        \n",
    "        all_results2.append([precision_score(y_true, y_pred),recall_score(y_true, y_pred),f1_score(y_true, y_pred)])\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
